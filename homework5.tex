\documentclass[12pt]{article}
\renewcommand*{\familydefault}{\sfdefault}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{fullpage}
\usepackage{tabularx}
\usepackage{graphicx}

\begin{document}
\title{CS540 Artificial Intelligence Homework 5}
\author{Yuchen Hou}
\maketitle

\section{Learning decision trees}

\subsection{a}
\begin{align*}
  H(Goal)
  &= -\sum_g P(Goal = g)lgP(Goal = g) \\
  &= -5/10*lg(5/10) -5/10*lg(5/10) \\
  &= 1
\end{align*}

\subsection{b}
\begin{align*}
  &Gain(Attribute)\\
  &= H(Goal) - H(Goal|Attribute) \\
  &H(Goal|Attribute)\\
  &= \sum_a P(Attribute=a) H(Goal|Attribute=a) \\
  &H(Goal|Cars)\\
  &= 3/10*(-0-0) + 4/10*(-1/4*lg(1/4)-3/4*lg(3/4)) + 3/10*(-1/3*lg(1/3)-2/3*lg(2/3)) \\
  &= 0.6 \\
  &H(Goal|Wheels)\\
  &= 4/10*(-1/4*lg(1/4)-3/4*lg(3/4)) + 1/10*(-0-0) + 5/10*(-2/5*lg(2/5)-3/5*lg(3/5)) \\
  &= 0.80998654701 \\
  &H(Goal|Circles)\\
  &= 2/10*(-1/2*lg(1/2)-1/2*lg(1/2)) + 5/10*(-3/5*lg(3/5)-2/5*lg(2/5)) + 2/10*(-1/2*lg(1/2)-1/2*lg(1/2)) + 1/10*(-0-0) \\
  &= 0.88547529722 \\
  &H(Goal|Cargo)\\
  &= 1/10*(-0-0) + 3/10*(-0-0) + 5/10*(-1/5*lg(1/5)-4/5*lg(4/5)) + 1/10*(-0-0) \\
  &= 0.36096404744 \\
  &Gain(Cars) = H(Goal) - H(Goal|Cars) = 1 - 0.6 = 0.4\\
  &Gain(Wheels) = H(Goal) - H(Goal|Wheels) = 1 - 0.80998654701 = 0.19001345299\\
  &Gain(Circles) = H(Goal) - H(Goal|Circles) = 1 - 0.88547529722 = 0.11452470278\\
  &Gain(Cargo) = H(Goal) - H(Goal|Cargo) = 1 - 0.36096404744 = 0.63903595256\\
\end{align*}

\subsection{c}
The final decision tree is shown in Figure \ref{fig:decision_tree}.
\begin{figure}
  \centering
      {\includegraphics[width=1\linewidth]{homework5_1.png}} \rule{1\linewidth}{1pt}
      \caption{The final decision tree}
      \label{fig:decision_tree}
\end{figure}

\subsection{d}
The procedure is as follows:
\begin{enumerate}
  \item check attribute cargo;
  \item check attribute circles;
  \item return east;
\end{enumerate}

\section{Regression and classification with linear models}

\subsection{a}
The learning process with perceptron weight update rule is shown in Table \ref{tab:perceptron}. The linear classification and weight update rules are shown below:
\begin{align*}
  o
  &= h(x)\\
  &= Threshold(w \cdot x)\\
  w_{t+1}
  &= w_t + \eta (y_t-o_t) \cdot x_t
\end{align*}
\subsection{b}
The final network would classify the instance d as 1, as shown in the last line in Table \ref{tab:perceptron}, which would be a mistake.
\begin{table}[htb]
  \centering
  \begin{tabularx}{\textwidth}{|l|l|l|l|l|X|} \hline
    example & w & x & y & o & $\Delta$ w = $\eta$ (y - o) $\cdot$ x \\ \hline
    a & (1,1,1,1,1) & (1,4,0,1,4) & 0 & 1 & 0.5*(0-1)(1,4,0,1,4) = (-0.5,-2,0,-0.5,-2) \\ \hline
    b & (0.5,-1,1,0.5,-1) & (1,3,2,2,3) & 0 & 0 & 0 \\ \hline
    c & (0.5,-1,1,0.5,-1) & (1,3,2,1,3) & 0 & 0 & 0 \\ \hline
    d & (0.5,-1,1,0.5,-1) & (1,4,1,0,3) & 0 & 0 & 0 \\ \hline
    e & (0.5,-1,1,0.5,-1) & (1,3,2,1,3) & 0 & 0 & 0 \\ \hline
    f & (0.5,-1,1,0.5,-1) & (1,2,2,3,2) & 1 & 1 & 0 \\ \hline
    g & (0.5,-1,1,0.5,-1) & (1,3,2,1,2) & 1 & 0 & 0.5*1*(1,3,2,1,2) = (0.5,1.5,1,0.5,1) \\ \hline
    h & (1,0.5,2,1,0) & (1,2,1,1,2) & 1 & 1 & 0 \\ \hline
    i & (1,0.5,2,1,0) & (1,4,1,2,3) & 1 & 1 & 0 \\ \hline
    j & (1,0.5,2,1,0) & (1,2,1,0,1) & 1 & 1 & 0 \\ \hline
    d & (1,0.5,2,1,0) & (1,4,1,1,3) & 0 & 1 & \\ \hline
  \end{tabularx}
  \caption{perceptron learning}
  \label{tab:perceptron}
\end{table}


\end{document}
